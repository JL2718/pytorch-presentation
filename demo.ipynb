{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a comprehensive PyTorch tutorial that demonstrates the use of several key features: `autograd`, `DataLoader`, `TensorBoard`, `profiler`, `TorchScript`, `quantization`, and `Captum`. We'll build a simple neural network to classify the MNIST dataset.\n",
    "\n",
    "### Step 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch.quantization\n",
    "from captum.attr import IntegratedGradients\n",
    "import torch.jit\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 2: Define the Model\n",
    "\n",
    "We'll define a simple feedforward neural network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3: Data Loading\n",
    "\n",
    "We'll use the `DataLoader` to load the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:06<00:00, 1587678.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 271254.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 532276.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1177557.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4: Training Loop\n",
    "\n",
    "We'll define a simple training loop and use `autograd` for backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2561\n",
      "Epoch 2, Loss: 0.1129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "for epoch in range(2):  # Train for 2 epochs\n",
    "    loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5: TensorBoard Integration\n",
    "\n",
    "We'll use TensorBoard to visualize the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "required keyword attribute 'name' has the wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[1;32m      5\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Log training loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m/nix/store/9bmi8pfb1vy6jw3v1ppqq7w5mc57pabb-python3-3.11.8-env/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:889\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    885\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\n\u001b[0;32m--> 889\u001b[0m         \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;66;03m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m/nix/store/9bmi8pfb1vy6jw3v1ppqq7w5mc57pabb-python3-3.11.8-env/lib/python3.11/site-packages/torch/utils/tensorboard/_pytorch_graph.py:340\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mprint\u001b[39m(graph)\n\u001b[0;32m--> 340\u001b[0m list_of_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# We are hardcoding that this was run on CPU even though it might have actually\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# run on GPU. Note this is what is shown in TensorBoard and has no bearing\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# on actual execution.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# and\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/step_stats.proto\u001b[39;00m\n\u001b[1;32m    351\u001b[0m stepstats \u001b[38;5;241m=\u001b[39m RunMetadata(\n\u001b[1;32m    352\u001b[0m     step_stats\u001b[38;5;241m=\u001b[39mStepStats(dev_stats\u001b[38;5;241m=\u001b[39m[DeviceStepStats(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/device:CPU:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m    353\u001b[0m )\n",
      "File \u001b[0;32m/nix/store/9bmi8pfb1vy6jw3v1ppqq7w5mc57pabb-python3-3.11.8-env/lib/python3.11/site-packages/torch/utils/tensorboard/_pytorch_graph.py:260\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(graph, trace, args, omit_useless_nodes)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mkind() \u001b[38;5;241m==\u001b[39m GETATTR_KIND:\n\u001b[0;32m--> 260\u001b[0m         attr_name \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m         attr_key \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39moutput()\u001b[38;5;241m.\u001b[39mdebugName()\n\u001b[1;32m    262\u001b[0m         parent \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39minput()\u001b[38;5;241m.\u001b[39mnode()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: required keyword attribute 'name' has the wrong type"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter('runs/mnist_experiment')\n",
    "\n",
    "# Log the model graph\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "writer.add_graph(model, images.to(device))\n",
    "\n",
    "# Log training loss\n",
    "for epoch in range(2):\n",
    "    loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    writer.add_scalar('training loss', loss, epoch)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Step 6: Profiling\n",
    "\n",
    "We'll use the profiler to analyze the performance of our model.\n",
    "\n",
    "```python\n",
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "```\n",
    "\n",
    "### Step 7: TorchScript\n",
    "\n",
    "We'll convert our model to TorchScript for deployment.\n",
    "\n",
    "```python\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"mnist_model.pt\")\n",
    "```\n",
    "\n",
    "### Step 8: Quantization\n",
    "\n",
    "We'll quantize our model to reduce its size and improve inference speed.\n",
    "\n",
    "```python\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"mnist_quantized_model.pt\")\n",
    "```\n",
    "\n",
    "### Step 9: Captum for Model Interpretability\n",
    "\n",
    "We'll use Captum to interpret the model's predictions.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Get a sample image\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "image = images[0].unsqueeze(0).to(device)\n",
    "\n",
    "# Compute attributions\n",
    "attributions, delta = ig.attribute(image, target=labels[0], return_convergence_delta=True)\n",
    "\n",
    "print('Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This tutorial demonstrated how to use several key features of PyTorch, including `autograd`, `DataLoader`, `TensorBoard`, `profiler`, `TorchScript`, `quantization`, and `Captum`. Each of these features plays a crucial role in developing, optimizing, and interpreting deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
