{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        10.83%       3.463ms       100.00%      31.980ms      31.980ms             1  \n",
      "                                           aten::linear         3.63%       1.161ms        31.50%      10.074ms       5.037ms             2  \n",
      "                                         aten::mse_loss         3.73%       1.194ms        22.05%       7.051ms       7.051ms             1  \n",
      "                                            aten::addmm        16.25%       5.198ms        20.11%       6.432ms       3.216ms             2  \n",
      "                                             aten::mean         3.42%       1.092ms        17.98%       5.749ms       5.749ms             1  \n",
      "autograd::engine::evaluate_function: MseLossBackward...         0.37%     119.832us        10.77%       3.443ms       3.443ms             1  \n",
      "                                       MseLossBackward0         3.11%     993.539us        10.39%       3.323ms       3.323ms             1  \n",
      "                                             aten::div_         5.58%       1.784ms         9.41%       3.010ms       3.010ms             1  \n",
      "                                aten::mse_loss_backward         4.30%       1.375ms         8.65%       2.768ms       1.384ms             2  \n",
      "                                                aten::t         3.37%       1.077ms         7.81%       2.496ms     277.356us             9  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 31.980ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create some random input data\n",
    "inputs = torch.randn(100, 10)\n",
    "targets = torch.randn(100, 1)\n",
    "\n",
    "# Profile the model\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Print the profiling results\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3740e-02],\n",
      "        [ 3.0118e-01],\n",
      "        [-1.5750e-01],\n",
      "        [-9.5050e-02],\n",
      "        [ 2.9309e-03],\n",
      "        [ 7.8783e-02],\n",
      "        [-4.0936e-02],\n",
      "        [ 3.6552e-02],\n",
      "        [ 2.1995e-01],\n",
      "        [-1.5086e-01],\n",
      "        [ 3.2749e-01],\n",
      "        [ 2.7731e-01],\n",
      "        [ 1.0149e-01],\n",
      "        [ 3.4689e-01],\n",
      "        [-1.7531e-01],\n",
      "        [ 6.1558e-02],\n",
      "        [ 4.7356e-01],\n",
      "        [-9.1545e-03],\n",
      "        [ 5.9004e-01],\n",
      "        [-2.1180e-01],\n",
      "        [ 3.7610e-01],\n",
      "        [ 8.7792e-02],\n",
      "        [ 6.3370e-01],\n",
      "        [-2.4760e-01],\n",
      "        [-1.3201e-01],\n",
      "        [ 2.0317e-01],\n",
      "        [ 2.4588e-01],\n",
      "        [ 3.9532e-01],\n",
      "        [ 1.0761e-01],\n",
      "        [-1.1565e-01],\n",
      "        [-2.5185e-01],\n",
      "        [-1.6885e-02],\n",
      "        [ 2.7115e-01],\n",
      "        [ 2.9114e-01],\n",
      "        [ 2.4828e-01],\n",
      "        [ 2.1044e-01],\n",
      "        [-1.7291e-01],\n",
      "        [ 3.5096e-01],\n",
      "        [-4.0652e-01],\n",
      "        [-1.3003e-01],\n",
      "        [ 4.2562e-01],\n",
      "        [ 2.4875e-01],\n",
      "        [ 3.0447e-02],\n",
      "        [ 3.2911e-01],\n",
      "        [-2.0681e-01],\n",
      "        [ 2.3139e-01],\n",
      "        [ 3.6410e-01],\n",
      "        [ 2.3833e-03],\n",
      "        [ 2.5763e-01],\n",
      "        [ 1.7898e-01],\n",
      "        [-2.7615e-01],\n",
      "        [ 1.2751e-01],\n",
      "        [ 3.4548e-01],\n",
      "        [ 2.5430e-06],\n",
      "        [ 1.0313e-01],\n",
      "        [ 3.5934e-02],\n",
      "        [ 9.5295e-02],\n",
      "        [ 1.3766e-01],\n",
      "        [ 5.7201e-02],\n",
      "        [ 4.1236e-02],\n",
      "        [ 1.6182e-01],\n",
      "        [-1.9794e-01],\n",
      "        [ 2.3584e-01],\n",
      "        [ 6.8126e-02],\n",
      "        [ 3.8105e-01],\n",
      "        [ 5.2483e-01],\n",
      "        [ 2.6071e-02],\n",
      "        [ 5.9122e-02],\n",
      "        [ 5.5056e-02],\n",
      "        [-1.3042e-01],\n",
      "        [ 1.4198e-01],\n",
      "        [-5.0725e-02],\n",
      "        [ 6.2626e-02],\n",
      "        [-1.4875e-01],\n",
      "        [ 4.5387e-02],\n",
      "        [-3.1221e-01],\n",
      "        [ 3.5330e-01],\n",
      "        [ 3.9902e-01],\n",
      "        [ 2.3818e-01],\n",
      "        [ 5.5170e-02],\n",
      "        [ 1.5704e-01],\n",
      "        [ 3.8108e-01],\n",
      "        [-1.4957e-01],\n",
      "        [ 1.5653e-01],\n",
      "        [ 3.4088e-01],\n",
      "        [ 3.8810e-01],\n",
      "        [ 2.2752e-01],\n",
      "        [-8.2011e-03],\n",
      "        [ 2.4372e-01],\n",
      "        [ 3.2738e-01],\n",
      "        [ 8.0769e-01],\n",
      "        [-1.4831e-01],\n",
      "        [ 1.6925e-02],\n",
      "        [ 2.8524e-01],\n",
      "        [ 6.4942e-02],\n",
      "        [ 2.7378e-01],\n",
      "        [ 3.2273e-01],\n",
      "        [ 1.8998e-01],\n",
      "        [ 1.9223e-01],\n",
      "        [ 1.6186e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Convert the model to TorchScript\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save the TorchScript model\n",
    "scripted_model.save(\"simple_model.pt\")\n",
    "\n",
    "# Load the TorchScript model\n",
    "loaded_model = torch.jit.load(\"simple_model.pt\")\n",
    "\n",
    "# Run inference with the loaded model\n",
    "inputs = torch.randn(100, 10)\n",
    "outputs = loaded_model(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "#include <torch/script.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main(int argc, const char* argv[]) {\n",
    "    if (argc != 2) {\n",
    "        std::cerr << \"Usage: example-app <path_to_model>\\n\";\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    torch::jit::script::Module module;\n",
    "    try {\n",
    "        module = torch::jit::load(argv[1]);\n",
    "    } catch (const c10::Error& e) {\n",
    "        std::cerr << \"Error loading model: \" << e.what() << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Create a sample input tensor\n",
    "    std::vector<torch::jit::IValue> inputs;\n",
    "    inputs.push_back(torch::randn({100,10}));\n",
    "\n",
    "    // Execute the model\n",
    "    torch::Tensor output = module.forward(inputs).toTensor();\n",
    "\n",
    "    std::cout << \"Output:\\n\" << output << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1317 [kernel]\nBackendSelect: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mconvert(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run inference with the quantized model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:168\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/bn18jq8d2wn0bvhdlb8d6hflglpj5kp1-python3-3.12.6-env/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1317 [kernel]\nBackendSelect: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /tmp/nix-build-python3.12-torch-2.4.1.drv-0/source/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Ensure the backend is supported\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "# Prepare the model for quantization\n",
    "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the model (e.g., with a few batches of data)\n",
    "inputs = torch.randn(100, 10)\n",
    "model(inputs)\n",
    "\n",
    "# Convert the model to a quantized model\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Run inference with the quantized model\n",
    "outputs = model(inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
